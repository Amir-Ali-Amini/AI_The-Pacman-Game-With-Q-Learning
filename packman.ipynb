{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AmirAli Amini\n",
    "610399102\n",
    "\n",
    "Report:\n",
    "\n",
    "STEP 1:\n",
    "    At first in (__init__) function we constructor method that initializes the Pacman object. It takes several parameters:\n",
    "        1. n and m are the dimensions of the Pacman board.\n",
    "        2. discount_factor is a value between 0 and 1 that determines the importance of future rewards in the Q-learning algorithm.\n",
    "        3. learning_rate is a value between 0 and 1 that determines the rate at which the Q-values are updated.\n",
    "        4. initial_state is the initial position of Pacman on the board.\n",
    "        5. rewards is a dictionary containing the rewards for each type of cell on the board.\n",
    "\n",
    "\n",
    "STEP 2:\n",
    "    Next function (input_board_matrix(self, matrix, initial_state=[0,1])): allows us to input a custom board matrix for the Pacman game. It takes a matrix and sets it as the game board and put a extera lelments around it. then compute the initial_state that parameter allows you to specify the initial position of Pacman on the board and the number of dots .\n",
    "\n",
    "\n",
    "STEP 3:\n",
    "    move(self, randomness=0.2, printing=False): This method is responsible for moving Pacman on the board. It takes two optional parameters:\n",
    "        1.randomness determines the probability of Pacman making a random move instead of following the Q-values.\n",
    "        2.printing is a boolean that controls whether to print the board after each move.\n",
    "    The method uses a Q-learning algorithm to update the Q-values based on the rewards and Pacman's movements. It returns the number of steps taken.\n",
    "\n",
    "\n",
    "STEP 4:\n",
    "    train(self, n): This method trains Pacman by repeatedly calling the move() method. It takes an integer n that represents the number of training iterations.\n",
    "\n",
    "\n",
    "STEP 5:\n",
    "    print_board(self, board, step): This method prints the current state of the board. It takes the board as a parameter and an optional step parameter to indicate the current step.\n",
    "\n",
    "\n",
    "STEP 6:\n",
    "    prnt(self): This method prints the current state of the board and the Q-matrix.\n",
    "\n",
    "\n",
    "\n",
    "finally These functions together create a Pacman game environment and provide methods for training and playing the game using a basic Q-learning algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pacman:\n",
    "    def __init__(self, n, m,discount_factor=0.25, learning_rate = 1 ,initial_state = [0,0] , rewards={\n",
    "            \"W\" :-10,\n",
    "            \"D\" : 1,\n",
    "            \"E\" : -0.1,\n",
    "            \"w\" :-10,\n",
    "            \"d\" : 1,\n",
    "            \"e\" : -0.1,\n",
    "            \"A\" : 0 ,\n",
    "            \"a\" : 0,\n",
    "            \"G\" : -10,\n",
    "            \"g\" : -10,\n",
    "            \"o\" : -10,\n",
    "            \"O\" : -10,\n",
    "        }):\n",
    "        # acitons:\n",
    "        #   down:0\n",
    "        #   right:1\n",
    "        #   up:2\n",
    "        #   left:3\n",
    "\n",
    "        self.n=n+2\n",
    "        self.m=m+2\n",
    "        self.board = np.array([[\"D\"]*(m+2 )for _ in range (n+2)])\n",
    "        self.q_matrix = np.array([[0]*(self.n * self.m) for _ in range (4)],dtype=float)\n",
    "        print(self.q_matrix.max(axis=0))\n",
    "\n",
    "        self.initial_state = initial_state\n",
    "        self.current_state = [0,0]\n",
    "        \n",
    "        self.discount_factor=discount_factor\n",
    "        self.learning_rate=learning_rate\n",
    "\n",
    "        self.number_of_dots = 0\n",
    "\n",
    "        self.rewards =rewards\n",
    "        self.rewards[\"o\"]=-10\n",
    "        self.rewards[\"O\"]=-10\n",
    "\n",
    "\n",
    "    def input_board_matrix(self, matrix , initial_state=[0,1]):\n",
    "        self.initial_state=[initial_state[0]+1,initial_state[1]+1 ]\n",
    "        for row in matrix:\n",
    "            row[0:0] = [\"O\"]\n",
    "            row.append(\"O\")\n",
    "        matrix[0:0] = [[\"O\"]*(self.m)]\n",
    "        matrix.append ([\"O\"]*(self.m))\n",
    "        for row in matrix:\n",
    "                print (row)\n",
    "        self.board = copy.copy(np.array(matrix))\n",
    "        for row in range(len(matrix)):\n",
    "            for column in range(len(matrix[row])):\n",
    "                if matrix[row][column] == \"D\" or matrix[row][column] == \"d\" :\n",
    "                    self.number_of_dots +=1\n",
    "                elif matrix[row][column] == \"A\" or matrix[row][column] == \"a\" :\n",
    "                    self.initial_state = [row,column]\n",
    "\n",
    "    def move(self,randomness=0.2,printing=False):\n",
    "        current_state = self.initial_state\n",
    "        dots = self.number_of_dots\n",
    "        board = copy.deepcopy(self.board)\n",
    "        current_move=0\n",
    "        step = 0\n",
    "        while (dots):\n",
    "            step +=1\n",
    "            action_reward = -1\n",
    "            tries = 10\n",
    "            next_state=[0,0]\n",
    "            while((action_reward<1 and 5<tries) or (action_reward<0 and tries)):\n",
    "                tries -=1\n",
    "                chance = random.random()\n",
    "                if chance<randomness : \n",
    "                    current_move:random.randint(0,3)\n",
    "                else :\n",
    "                    m = 0\n",
    "                    rewards = []\n",
    "                    next_state = [current_state[0]+1,current_state[1]]\n",
    "                    rewards.append(self.rewards[board[next_state[0],next_state[1]]])\n",
    "                    next_state = [current_state[0],current_state[1]+1]\n",
    "                    rewards.append(self.rewards[board[next_state[0],next_state[1]]])\n",
    "                    next_state = [current_state[0]-1,current_state[1]]\n",
    "                    rewards.append(self.rewards[board[next_state[0],next_state[1]]])\n",
    "                    next_state = [current_state[0],current_state[1]-1]\n",
    "                    rewards.append(self.rewards[board[next_state[0],next_state[1]]])\n",
    "                    column = self.q_matrix[:,current_state[0]*self.m + current_state[1]]\n",
    "                    # print(rewards)\n",
    "                    for j in range(1,4):\n",
    "                        if (column[j]+rewards[j]>= column[m]+rewards[m]) : m = j\n",
    "                    current_move= m\n",
    "                if (current_move== 0):\n",
    "                    next_state = [current_state[0]+1,current_state[1]]\n",
    "                elif (current_move== 1):\n",
    "                    next_state = [current_state[0],current_state[1]+1]\n",
    "                elif (current_move== 2):\n",
    "                    next_state = [current_state[0]-1,current_state[1]]\n",
    "                elif (current_move== 3):\n",
    "                    next_state = [current_state[0],current_state[1]-1]\n",
    "\n",
    "                # print (\"c_state: \" , current_state , \"n_state: \" , next_state, \"move: \", current_move)\n",
    "                action_reward = self.rewards[board[next_state[0],next_state[1]]]\n",
    "                break\n",
    "            addition_Q_value =   self.learning_rate*(action_reward + self.discount_factor*(self.q_matrix.max(axis=0)[next_state[0]*self.m +next_state[1]]))\n",
    "            self.q_matrix[current_move][current_state[0]*self.m + current_state[1]] +=addition_Q_value \n",
    "            # print(\"additional value : \", addition_Q_value,self.q_matrix[current_move][current_state[0]*self.m + current_state[1]])\n",
    "            if board[next_state[0],next_state[1]] ==\"D\" or board[next_state[0],next_state[1]] ==\"d\":\n",
    "                dots-=1\n",
    "            elif  board[next_state[0],next_state[1]] in ['o','O','g','G']:\n",
    "                if printing:self.print_board(board,step)\n",
    "                return step\n",
    "            #     break\n",
    "            if ( board[next_state[0],next_state[1]] not in ['w','W']):\n",
    "                board[current_state[0]][current_state[1]]=\"E\"\n",
    "                current_state = copy.copy(next_state)\n",
    "                board[current_state[0]][current_state[1]]=\"A\"\n",
    "                if (not dots):print ('dots ended')\n",
    "            # self.print_board(board,step)\n",
    "            if printing:self.print_board(board,step)\n",
    "\n",
    "        # print(board,\"\\n\\n\" )\n",
    "        return step\n",
    "        \n",
    "    def train(self,n):\n",
    "        for i in range(n) :\n",
    "            print ( f'iteration {i} ,    step : {self.move(randomness=(n-i)/(2*n) , printing=i < 10 or n-i<10)}')\n",
    "\n",
    "\n",
    "    def print_board (self,board,step):\n",
    "        print (\"step : \",step)\n",
    "        dic = {\n",
    "            \"W\":'✅',\n",
    "            \"D\":'⚪️',\n",
    "            \"E\":'⚫️',\n",
    "            \"A\":'💠',\n",
    "            \"O\":'⬛️',\n",
    "        }\n",
    "        for row in board :\n",
    "            print ([dic[i] for i in row] ,' ')\n",
    "        print('--------------------------------------------------------------------')\n",
    "\n",
    "    def prnt(self):\n",
    "        print ('board: ')\n",
    "        for row in self.board:\n",
    "            print(row)\n",
    "\n",
    "        result = []\n",
    "        dic = [\"👇\",\"👉\",\"👆\",\"👈\",\"✅\"]\n",
    "        for i in range (self.n * self.m):\n",
    "            m= 0\n",
    "            for j in range(1,4):\n",
    "                if (self.q_matrix[j][i]>self.q_matrix[m][i]) : m = j\n",
    "            result.append(dic[m] if self.q_matrix[m][i] else dic[4])\n",
    "        result = np.array(result)\n",
    "\n",
    "        print(result.reshape(self.n , -1))\n",
    "\n",
    "\n",
    "        # print ('q matrix: ')\n",
    "        # for i in range (40):\n",
    "        #     print (f'{i+1} : {self.q_matrix[0][i]} , {self.q_matrix[1][i]} , {self.q_matrix[2][i]} , {self.q_matrix[3][i]}')\n",
    "        # # print (self.q_matrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Pacman(5,9,learning_rate=0.2)\n",
    "game.input_board_matrix([\n",
    "    [\"A\",\"W\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"W\",],\n",
    "    [\"D\",\"W\",\"D\",\"D\",\"W\",\"D\",\"W\",\"D\",\"D\"],\n",
    "    [\"D\",\"D\",\"D\",\"D\",\"W\",\"D\",\"W\",\"D\",\"D\"],\n",
    "    [\"D\",\"D\",\"D\",\"D\",\"W\",\"D\",\"W\",\"D\",\"D\"],\n",
    "    [\"D\",\"D\",\"D\",\"D\",\"W\",\"D\",\"W\",\"D\",\"D\"],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.train(1000)\n",
    "game.prnt()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gamma value is equal to 0.25 and alpha value is equal to 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Pacman(3,6,learning_rate=0.5)\n",
    "test.input_board_matrix([\n",
    "    [\"A\",\"W\",\"D\",\"D\",\"D\",\"D\"],\n",
    "    [\"D\",\"W\",\"D\",\"D\",\"W\",\"D\"],\n",
    "    [\"D\",\"D\",\"D\",\"D\",\"W\",\"D\"],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.train(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.prnt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gamma value is equal to 0.25 and alpha value is equal to 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case_25 = Pacman(7,9,learning_rate=0.01,discount_factor=0.25 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case_25.input_board_matrix([\n",
    "    [\"A\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",],\n",
    "    [\"D\",\"W\",\"W\",\"W\",\"D\",\"W\",\"W\",\"W\",\"D\",],\n",
    "    [\"D\",\"W\",\"D\",\"D\",\"D\",\"D\",\"D\",\"W\",\"D\",],\n",
    "    [\"D\",\"D\",\"D\",\"W\",\"E\",\"W\",\"D\",\"D\",\"D\",],\n",
    "    [\"D\",\"W\",\"D\",\"W\",\"E\",\"W\",\"D\",\"W\",\"D\",],\n",
    "    [\"D\",\"W\",\"D\",\"D\",\"W\",\"D\",\"D\",\"W\",\"D\",],\n",
    "    [\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case_25.train(100)\n",
    "test_case_25.prnt()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gamma value is equal to 0.5 and alpha value is equal to 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gama_5 = Pacman(7,9 , learning_rate=0.01, discount_factor=0.5 )\n",
    "test_gama_5.input_board_matrix([\n",
    "    [\"A\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",],\n",
    "    [\"D\",\"W\",\"W\",\"W\",\"D\",\"W\",\"W\",\"W\",\"D\",],\n",
    "    [\"D\",\"W\",\"D\",\"D\",\"D\",\"D\",\"D\",\"W\",\"D\",],\n",
    "    [\"D\",\"D\",\"D\",\"W\",\"E\",\"W\",\"D\",\"D\",\"D\",],\n",
    "    [\"D\",\"W\",\"D\",\"W\",\"E\",\"W\",\"D\",\"W\",\"D\",],\n",
    "    [\"D\",\"W\",\"D\",\"D\",\"W\",\"D\",\"D\",\"W\",\"D\",],\n",
    "    [\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gama_5.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gama_5.prnt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gamma value is equal to 1 and alpha value is equal to 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gama_1 = Pacman(7,9 , learning_rate=0.02, discount_factor=1 )\n",
    "test_gama_1.input_board_matrix([\n",
    "    [\"A\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",],\n",
    "    [\"D\",\"W\",\"W\",\"W\",\"D\",\"W\",\"W\",\"W\",\"D\",],\n",
    "    [\"D\",\"W\",\"D\",\"D\",\"D\",\"D\",\"D\",\"W\",\"D\",],\n",
    "    [\"D\",\"D\",\"D\",\"W\",\"E\",\"W\",\"D\",\"D\",\"D\",],\n",
    "    [\"D\",\"W\",\"D\",\"W\",\"E\",\"W\",\"D\",\"W\",\"D\",],\n",
    "    [\"D\",\"W\",\"D\",\"D\",\"W\",\"D\",\"D\",\"W\",\"D\",],\n",
    "    [\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gama_1.prnt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(game.q_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain what the number of states depends on. Can the number of states be reduced?\n",
    "\n",
    "The number of states is depends on the size of the game board. in my code the total number of states in the Pacman game environment is (n + 2) * (m + 2).Each state corresponds to a specific cell on the board, including the boundary cells.\n",
    "now we can reduce the number of states by considering only the inner cells of the board as valid states and omite extera walls that we add to environment at first function(def input_board_matrix(self, matrix , initial_state=[0,1])).\n",
    "By reducing the number of states, you can potentially simplify the game environment and make the learning process more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define Action, State, Rewards, and Goal State for the problem and provide your explanation.\n",
    "\n",
    "Action: An action represents a possible move or decision that Pacman can take in the game. In this case, the actions are limited to moving up, down, left, or right on the game board. Each action can lead Pacman from one state to another.\n",
    "\n",
    "State: A state represents the current configuration or position of Pacman on the game board. It captures the relevant information needed to make decisions and determine the next action. In this case, a state can be represented by the coordinates (row, column) that indicate the position of Pacman on the board.\n",
    "\n",
    "Rewards: Rewards are values associated with each state-action pair in the game. They represent the immediate benefit or penalty Pacman receives for taking a particular action in a specific state. Rewards can be positive for desirable actions (e.g., eating a dot) and negative for unfavorable actions (e.g., hitting a wall). The rewards can be defined as a dictionary, where each state-action pair maps to a reward value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(Q_table,n,m):\n",
    "        matrix = [[0]*(n*m) for _ in range (n*m)]\n",
    "        for i in range (1,n-1):\n",
    "            for j in range (1,m-1):\n",
    "                currentState = [i,j]\n",
    "                matrix[(i)*m+j ][(i+1)*m+j ] = Q_table[0][i*m+j]\n",
    "                matrix[(i)*m+j ][(i)*m+j+1 ] = Q_table[1][i*m+j]\n",
    "                matrix[(i)*m+j ][(i-1)*m+j ] = Q_table[2][i*m+j]\n",
    "                matrix[(i)*m+j ][(i)*m+j-1 ] = Q_table[3][i*m+j]\n",
    "        matrix = np.array(matrix)\n",
    "\n",
    "        G = nx.from_numpy_array(matrix)\n",
    "\n",
    "        # Specify the layout for better visualization (you can try other layouts as well)\n",
    "        pos = nx.spring_layout(G)\n",
    "\n",
    "        # Draw the graph with edge labels\n",
    "        nx.draw(G, pos, with_labels=True, font_weight='bold', node_size=700, node_color='skyblue', font_size=8)\n",
    "        labels = nx.get_edge_attributes(G, \"weight\")\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph=Pacman(7,9,learning_rate=0.01)\n",
    "test_graph.input_board_matrix([\n",
    "    [\"A\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",],\n",
    "    [\"D\",\"W\",\"W\",\"W\",\"D\",\"W\",\"W\",\"W\",\"D\",],\n",
    "    [\"D\",\"W\",\"D\",\"D\",\"D\",\"D\",\"D\",\"W\",\"D\",],\n",
    "    [\"D\",\"D\",\"D\",\"W\",\"E\",\"W\",\"D\",\"D\",\"D\",],\n",
    "    [\"D\",\"W\",\"D\",\"W\",\"E\",\"W\",\"D\",\"W\",\"D\",],\n",
    "    [\"D\",\"W\",\"D\",\"D\",\"W\",\"D\",\"D\",\"W\",\"D\",],\n",
    "    [\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",\"D\",],\n",
    "])\n",
    "test_graph.train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table, n , m = test_graph.q_matrix,test_graph.n,test_graph.m\n",
    "graph(Q_table, n , m )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
